{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b7bbdfe-a911-4a14-8d07-dc5c2782f637",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fa73f31-193d-453b-9530-271e12f0dbd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "import re\n",
    "import pyspark.sql.types as T\n",
    "from math import ceil\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Type, Any, Union\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from common import get_datestr\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e49f81a-7915-4ba9-8252-c184631cf910",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run Shared/Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "385f7ec0-f61b-48b6-8e2c-aa6e8fb4ddbf",
     "showTitle": true,
     "title": "Metadata (eventually embedded directly into the parquet dataframe through schema)"
    }
   },
   "outputs": [],
   "source": [
    "def load_hubspot_metadata(sheet_name):\n",
    "    # put the whole excel into dataframe metadata because why not ¯\\_(ツ)_/¯\n",
    "\n",
    "    local_path = Path('')\n",
    "    REFERENCE_PATH = APPLICATION_PATH / 'hubspot' / ''\n",
    "\n",
    "    REFERENCE_PATH.cp('file:' + str(local_path))\n",
    "\n",
    "    objects_properties_reference = pd.read_excel(local_path, sheet_name=sheet_name)\n",
    "\n",
    "    metadata = defaultdict(lambda: {})\n",
    "\n",
    "    for index, row in objects_properties_reference.iterrows():\n",
    "        keys = row.index.values.tolist()\n",
    "\n",
    "        keys.remove('Internal name')\n",
    "\n",
    "        metadata[row['Internal name']] = {\n",
    "            key.replace(' ', '_').lower(): str(row[key]) for key in keys\n",
    "        }\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def add_custom_metadata(metadata, property_type_counts, property_schemas):\n",
    "    # add more metadata that might help understand/debug\n",
    "    keys = list(set(property_type_counts.keys()).union(set(property_schemas.keys())))\n",
    "\n",
    "    for key in keys:\n",
    "        assert 'type_counts' not in metadata[key]\n",
    "        assert 'property_schema' not in metadata[key]\n",
    "\n",
    "        if key in property_type_counts:\n",
    "            metadata[key]['type_counts'] = str(dict(property_type_counts[key])) # defaultdict -> dict conversion, otherwise the string is ugly as hell\n",
    "        \n",
    "        if key in property_schemas:\n",
    "            metadata[key]['property_schema'] = str(property_schemas[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee38c635-7198-4721-b40e-84bfa60a3453",
     "showTitle": true,
     "title": "Load json queue"
    }
   },
   "outputs": [],
   "source": [
    "def load_json_queue(object_name):\n",
    "    path = RAW_PATH / 'hubspot' / object_name\n",
    "\n",
    "    json_file_queue = []\n",
    "\n",
    "    for folder in path.iterdir():\n",
    "        # print(folder)\n",
    "        for file in folder.reiterdir(r'.*\\.json'):\n",
    "            record = {\n",
    "                'path': file,\n",
    "            }\n",
    "\n",
    "            if re.match(r'^\\d{8}$', folder.name):\n",
    "                record['load_datetime'] = datetime.strptime(folder.name, '%Y%m%d')\n",
    "            elif re.match(r'^\\d{8}_\\d{6}$', folder.name):\n",
    "                record['load_datetime'] = datetime.strptime(folder.name, '%Y%m%d_%H%M%S')\n",
    "            else:\n",
    "                raise ValueError(f'unparsable folder name: {folder.name}')\n",
    "\n",
    "            json_file_queue.append(record)\n",
    "            #print(record)\n",
    "\n",
    "    return json_file_queue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df3fc2b-e7e5-403b-af3f-80e2aaa4605e",
     "showTitle": true,
     "title": "Count Types"
    }
   },
   "outputs": [],
   "source": [
    "def collect_properties(data):\n",
    "    # helper function that just loads all data from a json and collects it by column instead of by row\n",
    "    properties = defaultdict(lambda: [])\n",
    "\n",
    "    def add_property(properties, key, value):\n",
    "        if isinstance(value, dict): # nested property\n",
    "            try:\n",
    "                properties[key].append(value['value'])\n",
    "            except:\n",
    "                print(value)\n",
    "                raise\n",
    "        else: # flat property\n",
    "            properties[key].append(value)\n",
    "\n",
    "    for record in data:\n",
    "        for key, value in record.items():\n",
    "            if key != 'properties':\n",
    "                # mark all top-level properties as meta-properties to clearly distinguish them from the 'properties' list\n",
    "                key = f'meta-{key}'\n",
    "\n",
    "                add_property(properties, key, value)\n",
    "        \n",
    "        for key, value in record['properties'].items():\n",
    "            add_property(properties, key, value)\n",
    "\n",
    "    return properties\n",
    "\n",
    "is_null = re.compile(r'^$')\n",
    "is_timestamp = re.compile(r'^\\d{13}$')\n",
    "\n",
    "\n",
    "def count_types_in_single_file(key, values, is_meta):\n",
    "    # count how many different types are encountered for every column\n",
    "    # uses several heuristics to guess/infer what data type it might be\n",
    "    # each data type is described by a string, to make further processing easier\n",
    "\n",
    "    python_types = {type(value) for value in values}\n",
    "\n",
    "    python_types_text = {str(t.__name__) for t in python_types}\n",
    "\n",
    "    if len(python_types) > 1 and 'NoneType' not in python_types_text:\n",
    "        raise ValueError(f'cannot parse {key} because it contains multiple python data types {python_types_text=}')\n",
    "    \n",
    "    type_counts = defaultdict(lambda: 0)\n",
    "\n",
    "    if is_meta:\n",
    "        if len(python_types) > 1:\n",
    "            assert False\n",
    "\n",
    "        dtype = list(python_types)[0]\n",
    "        \n",
    "        if dtype == list:\n",
    "            item_types = {type(elem) for item in values for elem in item}\n",
    "\n",
    "            if len(item_types) == 0:\n",
    "                type_counts['list<unknown>'] = len(values)\n",
    "            elif len(item_types) == 1:\n",
    "                item_dtype = list(item_types)[0]\n",
    "                type_counts[f'list<{item_dtype.__name__}>'] = len(values)\n",
    "            else:\n",
    "                raise ValueError(f'cannot parse {key} because it contains multiple python data types {python_types_text=}')\n",
    "        else:\n",
    "            type_counts[dtype.__name__] = len(values)\n",
    "\n",
    "    else:\n",
    "        if len(python_types_text - {'str', 'NoneType'}) > 0:\n",
    "            raise TypeError(f'cannot parse property {key} because it contains non-string values. {python_types_text=}')\n",
    "\n",
    "        for value in values:\n",
    "            if value is None or is_null.match(value):\n",
    "                type_counts['null'] += 1\n",
    "            elif is_timestamp.match(value):\n",
    "                type_counts['timestamp'] += 1\n",
    "            else:\n",
    "                try:\n",
    "                    val = int(value)\n",
    "\n",
    "                    if val > 9.0e+18: # it's int but it's out of bounds for java long\n",
    "                        type_counts['str'] += 1\n",
    "                    else:\n",
    "                        type_counts['int'] += 1\n",
    "                except:\n",
    "                    try:\n",
    "                        float(value)\n",
    "                        type_counts['float'] += 1\n",
    "                    except:\n",
    "                        if value in ['true', 'false']:\n",
    "                            type_counts['bool'] += 1\n",
    "                        else:\n",
    "                            type_counts['str'] += 1\n",
    "\n",
    "    return type_counts\n",
    "\n",
    "def count_types(queue):\n",
    "    # iterate over all files and count types for every column encountered\n",
    "    property_type_counts = defaultdict(lambda: [])\n",
    "\n",
    "    for queue_data in tqdm(queue):\n",
    "        data = json.loads(queue_data['path'].read_text())\n",
    "        properties = collect_properties(data)\n",
    "\n",
    "        for name, values in properties.items():\n",
    "            is_meta = name.startswith('meta-')\n",
    "            type_counts = count_types_in_single_file(name, values, is_meta)\n",
    "            property_type_counts[name].append(type_counts)\n",
    "\n",
    "    # merge type counts from different files into one\n",
    "    final_property_type_counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    for key in property_type_counts:\n",
    "        for type_counts in property_type_counts[key]:\n",
    "            for key2, value in type_counts.items():\n",
    "                final_property_type_counts[key][key2] += value\n",
    "\n",
    "    return final_property_type_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73b7f4cf-0d12-4f37-a01f-b259172bad48",
     "showTitle": true,
     "title": "Infer Data Types from Type Counts"
    }
   },
   "outputs": [],
   "source": [
    "# helper class to hold all information about a single column to be parsed from hubspot\n",
    "@dataclass\n",
    "class PropertySchema:\n",
    "    dtype: str # int, float, str, bool, list, datetime, date\n",
    "    is_meta: bool # meta-properties are top-level properties, regular properties occur inside 'properties' key\n",
    "    ignore_property: bool = False # allows filtering out unparsable columns without crashing the pipeline by raising an Error\n",
    "\n",
    "def infer_data_type(name, type_counts, is_meta, raise_unknown_types=True):\n",
    "    # logic that takes type counts and infers what actual data type it should be parsed as\n",
    "    if is_meta:\n",
    "        # this is incredibly messy, what it does is simply: allow only int, float, bool, str, list<int>, list<float>, list<bool>, list<str>\n",
    "        # this will be refactored later\n",
    "        nonzero_keys = []\n",
    "        for key, count in type_counts.items():\n",
    "            if count > 0:\n",
    "                nonzero_keys.append(key)\n",
    "\n",
    "        if len(nonzero_keys) > 2:\n",
    "            raise ValueError(f'cannot infer data type of meta property {name} with type counts: {type_counts}')\n",
    "        elif len(nonzero_keys) == 2:\n",
    "            if 'list<unknown>' not in nonzero_keys:\n",
    "                raise ValueError(f'cannot infer data type of meta property {name} with type counts: {type_counts}')\n",
    "\n",
    "            nonzero_keys.remove('list<unknown>')\n",
    "            if not nonzero_keys[0].startswith('list<'):\n",
    "                raise ValueError(f'cannot infer data type of meta property {name} with type counts: {type_counts}')\n",
    "\n",
    "            if nonzero_keys[0] not in ['list<int>', 'list<str>', 'list<float>', 'list<bool>']:\n",
    "                if raise_unknown_types:\n",
    "                    raise ValueError(f'cannot infer data type of meta property {name} with type counts: {type_counts}')\n",
    "                else:\n",
    "                    return PropertySchema('unknown', is_meta=True, ignore_property=True)\n",
    "\n",
    "            return PropertySchema(nonzero_keys[0], is_meta=True)\n",
    "        elif len(nonzero_keys) == 1:\n",
    "            if 'list<unknown>' in nonzero_keys:\n",
    "                if raise_unknown_types:\n",
    "                    raise ValueError(f'cannot infer data type of meta property {name} with type counts: {type_counts}')\n",
    "                else:\n",
    "                    return PropertySchema('unknown', is_meta=True, ignore_property=True)\n",
    "\n",
    "            if nonzero_keys[0].startswith('list') and nonzero_keys[0] not in ['list<int>', 'list<str>', 'list<float>', 'list<bool>']:\n",
    "                if raise_unknown_types:\n",
    "                    raise ValueError(f'cannot infer data type of meta property {name} with type counts: {type_counts}')\n",
    "                else:\n",
    "                    return PropertySchema('unknown', is_meta=True, ignore_property=True)\n",
    "\n",
    "            return PropertySchema(nonzero_keys[0], is_meta=True)\n",
    "        else:\n",
    "            raise ValueError(f'invalid state, find a bug above')\n",
    "    else:\n",
    "\n",
    "        def has(key):\n",
    "            return type_counts[key] > 0\n",
    "\n",
    "        # def has_re(key_regexp):\n",
    "            # return any([type_counts[key] > 0 for key in type_counts if re.match(key_regexp, key)])\n",
    "\n",
    "        def has_any_other_than(key):\n",
    "            return any(type_counts[key2] > 0 for key2 in type_counts if key2 != key)\n",
    "\n",
    "        if has('str'):\n",
    "            return PropertySchema('str', is_meta=False)\n",
    "\n",
    "        if has('bool'):\n",
    "            if has_any_other_than('bool'): # inconsistent entries, cannot be cleanly parsed\n",
    "                return PropertySchema('str', is_meta=False)\n",
    "\n",
    "            return PropertySchema('bool', is_meta=False)\n",
    "\n",
    "        if has('float'):\n",
    "            # timestamp, int & null entries are fine and parsable\n",
    "            return PropertySchema('float', is_meta=False)\n",
    "        \n",
    "        if has('int'):\n",
    "            # timestamp & null entries are fine and parsable\n",
    "            return PropertySchema('int', is_meta=False)\n",
    "\n",
    "        if has('timestamp'):\n",
    "            # null entries are fine and parsable\n",
    "            return PropertySchema('timestamp', is_meta=False)\n",
    "\n",
    "        if has('null'):\n",
    "            if raise_unknown_types:\n",
    "                raise ValueError(f'Unable to infer valid type for {name} from {type_counts}')\n",
    "            else:\n",
    "                return PropertySchema('unknown', is_meta=True, ignore_property=True)\n",
    "\n",
    "        raise ValueError(f'Unable to infer valid type for {name} from {type_counts}')\n",
    "\n",
    "\n",
    "def infer_property_schemas(property_type_counts):\n",
    "\n",
    "    property_schemas = {}\n",
    "\n",
    "    for name, type_counts in property_type_counts.items():\n",
    "        is_meta = name.startswith('meta-')\n",
    "\n",
    "        property_schema = infer_data_type(name, type_counts, is_meta, raise_unknown_types=False)\n",
    "\n",
    "        property_schemas[name] = property_schema\n",
    "            \n",
    "    return property_schemas\n",
    "\n",
    "\n",
    "def remove_ignored_property_schemas(property_schemas):\n",
    "\n",
    "    keys = list(property_schemas.keys())\n",
    "\n",
    "    for key in keys:\n",
    "        if property_schemas[key].ignore_property:\n",
    "            print(f'{key}: IGNORING PROPERTY AND DELETING IT FROM SCHEMA')\n",
    "            del property_schemas[key]\n",
    "\n",
    "    print(f'{len(property_schemas)} properties left')\n",
    "\n",
    "\n",
    "def save_property_schemas(property_schemas, path):\n",
    "    path.write_bytes(pickle.dumps(property_schemas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1436bd21-f3fe-42cb-a4a3-ba46ecb2c8aa",
     "showTitle": true,
     "title": "Build Schema"
    }
   },
   "outputs": [],
   "source": [
    "def parse_dtype(dtype_str):\n",
    "    if re.match('^list<.*>$', dtype_str):\n",
    "        return T.ArrayType(parse_dtype(re.match('^list<(.*)>$', dtype_str)[1]))\n",
    "    elif dtype_str == 'int':\n",
    "        return T.LongType()\n",
    "    elif dtype_str == 'float':\n",
    "        return T.DoubleType()\n",
    "    elif dtype_str == 'str':\n",
    "        return T.StringType()\n",
    "    elif dtype_str == 'bool':\n",
    "        return T.BooleanType()\n",
    "    elif dtype_str == 'timestamp':\n",
    "        return T.TimestampType()\n",
    "    else:\n",
    "        raise NotImplementedError(dtype_str)\n",
    "\n",
    "def build_pyspark_schema(property_schemas, metadata):\n",
    "    _metadata = defaultdict(lambda: {})\n",
    "    _metadata.update(metadata)\n",
    "\n",
    "    schema_suffix = [T.StructField('__load_timestamp', T.TimestampType(), False)]\n",
    "\n",
    "    fields = []\n",
    "    for name, schema in property_schemas.items():\n",
    "        if schema.ignore_property:\n",
    "            continue\n",
    "\n",
    "        fields.append(\n",
    "            T.StructField(name, parse_dtype(schema.dtype), True, _metadata[name])\n",
    "        )\n",
    "\n",
    "    return T.StructType(fields + schema_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b52fe36-f89f-406f-be3d-70a507c57263",
     "showTitle": true,
     "title": "Parse Jsons"
    }
   },
   "outputs": [],
   "source": [
    "def parse_meta_property(data, dtype):\n",
    "    if dtype in ['int', 'float', 'str', 'bool']:\n",
    "        return data\n",
    "    elif dtype.startswith('list<'):\n",
    "        return data\n",
    "    elif dtype == 'timestamp':\n",
    "        return datetime.fromtimestamp(data / 1000)\n",
    "    else:\n",
    "        raise TypeError(f'unknown schema {dtype=} for meta property')\n",
    "\n",
    "\n",
    "def parse_property(data, dtype):\n",
    "    if isinstance(data, dict):\n",
    "        data = data['value']\n",
    "    if data is None or data == '':\n",
    "        return None\n",
    "    elif dtype == 'str':\n",
    "        return data\n",
    "    elif dtype == 'timestamp':\n",
    "        return datetime.fromtimestamp(int(data) / 1000)\n",
    "    elif dtype == 'int':\n",
    "        return int(data)\n",
    "    elif dtype == 'float':\n",
    "        return float(data)\n",
    "    elif dtype == 'bool':\n",
    "        return data == 'true'\n",
    "    else:\n",
    "        raise TypeError(f'unknown {dtype=} for property')\n",
    "\n",
    "\n",
    "def parse_json(data, property_schemas, out_rows, row_suffix):\n",
    "    for record in data:\n",
    "        row = []\n",
    "        for key, schema in property_schemas.items():\n",
    "            if schema.is_meta:\n",
    "                if key in record:\n",
    "                    row.append(parse_meta_property(record[key], schema.dtype))\n",
    "                else:\n",
    "                    row.append(None)\n",
    "            else:\n",
    "                if key in record['properties']:\n",
    "                    row.append(parse_property(record['properties'][key], schema.dtype))\n",
    "                else:\n",
    "                    row.append(None)\n",
    "        \n",
    "        out_rows.append(row + row_suffix)\n",
    "\n",
    "\n",
    "def parse_jsons(queue, property_schemas, pyspark_schema, intermediate_path):\n",
    "    first_chunk = True\n",
    "\n",
    "    out_data = []\n",
    "\n",
    "    for queue_data in tqdm(queue):\n",
    "        data = json.loads(queue_data['path'].read_text())\n",
    "\n",
    "        row_suffix = [queue_data['load_datetime']] # appended to every row\n",
    "        parse_json(data, property_schemas, out_data, row_suffix)\n",
    "\n",
    "        if len(out_data) > 20000:\n",
    "            if first_chunk:\n",
    "                mode = 'overwrite'\n",
    "            else:\n",
    "                mode = 'append'\n",
    "\n",
    "            first_chunk = False\n",
    "\n",
    "            df = spark.createDataFrame(out_data, schema=pyspark_schema).repartition(1)\n",
    "            df.write.mode(mode).parquet(str(intermediate_path))\n",
    "            # print(f'wrote {len(out_data)} rows with {mode=} to {str(intermediate_path)}...')\n",
    "\n",
    "            out_data = []\n",
    "\n",
    "    if first_chunk:\n",
    "        mode = 'overwrite'\n",
    "    else:\n",
    "        mode = 'append'\n",
    "\n",
    "    first_chunk = False\n",
    "\n",
    "    df = spark.createDataFrame(out_data, schema=pyspark_schema).repartition(1)\n",
    "    # print(f'wrote {len(out_data)} rows with {mode=} to {str(intermediate_path)}...')\n",
    "    df.write.mode(mode).parquet(str(intermediate_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f20a5f2-7a3f-43c9-bc2d-75c6e87673ae",
     "showTitle": true,
     "title": "Deduplicate and finalize"
    }
   },
   "outputs": [],
   "source": [
    "def deduplicate_and_finalize(intermediate_path, out_path):\n",
    "    df = spark.read.parquet(str(intermediate_path)).cache()\n",
    "\n",
    "    load_meta_columns = ['__load_timestamp']\n",
    "\n",
    "    deduplication_cols = [col for col in df.columns if col not in load_meta_columns]\n",
    "    deduplication_sort_cols = ['__load_timestamp']\n",
    "\n",
    "    deduplication_group_window = Window.partitionBy(deduplication_cols).orderBy(deduplication_sort_cols).rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "    num_before_deduplication = df.count()\n",
    "\n",
    "    deduplicated_df = (\n",
    "        df\n",
    "        .withColumn('duplicate_index', F.row_number().over(deduplication_group_window))\n",
    "        .where('duplicate_index == 1')\n",
    "        .drop('duplicate_index')\n",
    "        .cache()\n",
    "    )\n",
    "\n",
    "    num_after_deduplication = deduplicated_df.count()\n",
    "\n",
    "    fields_per_partition = 150 * 50000.0 # cols * rows\n",
    "\n",
    "    num_partitions = int(ceil(len(deduplicated_df.columns) * 1.0 * num_after_deduplication / fields_per_partition))\n",
    "\n",
    "    deduplicated_df = deduplicated_df.repartition(num_partitions)\n",
    "    deduplicated_df = deduplicated_df.write.mode('overwrite').parquet(str(out_path))\n",
    "\n",
    "    print(f'deduplicated from {num_before_deduplication} to {num_after_deduplication}, into {num_partitions=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "737115c4-a4b5-425e-ade8-14b40f1516ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cleanup(intermediate_path):\n",
    "    intermediate_path.rm(recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36002be5-fc77-48f3-9cba-3e3954c792d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "text_classification_using_various_modelsdef standardize(object_name, metadata_sheet, datestr):\n",
    "\n",
    "    intermediate_path = STANDARDIZED_PATH / 'hubspot' / f'{object_name}' / datestr / '_temp_intermediate_data'\n",
    "    output_path = STANDARDIZED_PATH / 'hubspot' / f'{object_name}' / datestr / 'data'\n",
    "    property_schemas_path = STANDARDIZED_PATH / 'hubspot' / f'{object_name}' / datestr / 'property_schemas.pkl'\n",
    "    \n",
    "    print(f'\\n\\n\\n---{object_name}: ⏳ STARTING ⏳ ---')\n",
    "    print(f'---{object_name}: LOADING METADATA---')\n",
    "    metadata = load_hubspot_metadata(metadata_sheet)\n",
    "    print(f'---{object_name}: LOADING QUEUE---')\n",
    "    queue = load_json_queue(object_name)\n",
    "    print(f'---{object_name}: COUNTING PROPERTY TYPES---')\n",
    "    property_type_counts = count_types(queue)\n",
    "    print(f'---{object_name}: INFERRING PROPERTY SCHEMA---')\n",
    "    property_schemas = infer_property_schemas(property_type_counts)\n",
    "    print(f'---{object_name}: INCLUDING ADDITIONAL METADATA---')\n",
    "    add_custom_metadata(metadata, property_type_counts, property_schemas)\n",
    "    print(f'---{object_name}: SAVING PROPERTY SCHEMA---')\n",
    "    save_property_schemas(property_schemas, property_schemas_path)\n",
    "    print(f'---{object_name}: REMOVING IGNORED PROPERTIES---')\n",
    "    remove_ignored_property_schemas(property_schemas)\n",
    "    print(f'---{object_name}: BUILDING PYSPARK SCHEMA---')\n",
    "    pyspark_schema = build_pyspark_schema(property_schemas, metadata)\n",
    "    print(f'---{object_name}: PARSING JSONS INTO INTERMEDIATE PARQUET---')\n",
    "    parse_jsons(queue, property_schemas, pyspark_schema, intermediate_path)\n",
    "    print(f'---{object_name}: DEDUPLICATION INTO FINAL PARQUET---')\n",
    "    deduplicate_and_finalize(intermediate_path, output_path)\n",
    "    print(f'---{object_name}: CLEANUP INTERMEDIATE PARQUET---')\n",
    "    cleanup(intermediate_path)\n",
    "    print(f'---{object_name}: ✅ SUCCESFULLY FINISHED ✅ ---\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "272527fe-f1f6-465f-a491-71a61d79a11b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datestr = get_datestr()#datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "config = [\n",
    "    ('contacts', 'CONTACT'),\n",
    "    ('companies', 'COMPANY'),\n",
    "    ('input_material_streams', 'input_material_streams'),\n",
    "    ('output_material_streams', 'output_material_streams'),\n",
    "]\n",
    "\n",
    "for object_name, metadata_sheet in config:\n",
    "    standardize(object_name, metadata_sheet, datestr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f58381a5-f938-4c11-bcd5-7a086a7a265f",
     "showTitle": true,
     "title": "Testovaci kod 👇"
    }
   },
   "outputs": [],
   "source": [
    "# object_name = 'input_material_streams'\n",
    "# metadata_sheet = 'input_material_streams'\n",
    "# datestr = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "# # standardized -> standardized_testing, just to be safe\n",
    "# intermediate_path = \n",
    "# output_path = \n",
    "# property_schemas_path = \n",
    "\n",
    "# print(f'---{object_name}: ⏳ STARTING ⏳ ---')\n",
    "# print(f'---{object_name}: LOADING METADATA---')\n",
    "# metadata = load_hubspot_metadata(metadata_sheet)\n",
    "# print(f'---{object_name}: LOADING QUEUE---')\n",
    "# queue = load_json_queue(object_name)[-20:]\n",
    "# print(f'---{object_name}: COUNTING PROPERTY TYPES---')\n",
    "# property_type_counts = count_types(queue)\n",
    "# print(f'---{object_name}: INFERRING PROPERTY SCHEMA---')\n",
    "# property_schemas = infer_property_schemas(property_type_counts)\n",
    "# print(f'---{object_name}: INCLUDING ADDITIONAL METADATA---')\n",
    "# add_custom_metadata(metadata, property_type_counts, property_schemas)\n",
    "# print(f'---{object_name}: SAVING PROPERTY SCHEMA---')\n",
    "# save_property_schemas(property_schemas, property_schemas_path)\n",
    "# print(f'---{object_name}: REMOVING IGNORED PROPERTIES---')\n",
    "# remove_ignored_property_schemas(property_schemas)\n",
    "# print(f'---{object_name}: BUILDING PYSPARK SCHEMA---')\n",
    "# pyspark_schema = build_pyspark_schema(property_schemas, metadata)\n",
    "# print(f'---{object_name}: PARSING JSONS INTO INTERMEDIATE PARQUET---')\n",
    "# parse_jsons(queue, property_schemas, pyspark_schema, intermediate_path)\n",
    "# print(f'---{object_name}: DEDUPLICATION INTO FINAL PARQUET---')\n",
    "# deduplicate_and_finalize(intermediate_path, output_path)\n",
    "# print(f'---{object_name}: CLEANUP INTERMEDIATE PARQUET---')\n",
    "# cleanup(intermediate_path)\n",
    "# print(f'---{object_name}: ✅ SUCCESFULLY FINISHED ✅ ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "127e6af8-e91f-41e3-8816-6a7690305ec9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df = spark.read.parquet(str(output_path)).cache()\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d2bce37-f549-48bf-8fa7-0bab46be7708",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# props = collect_properties(json.loads(queue[0]['path'].read_text()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e3dec80-3039-4ebb-8590-f07895eb7e83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# data = json.loads(queue[0]['path'].read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "260ed47e-cda0-48f2-8cdc-119d23020b69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "def inspect_queue(queue, print_first_n=1, reversed_order=False):\n",
    "    \"\"\"\n",
    "    Helper function for seeing formatted json contents of the queue. It will print first n json records. It will load\n",
    "    as many jsons from the queue as necessary to print this number of records.\n",
    "\n",
    "    queue: list of records with path to jsons\n",
    "    print_first_n: \n",
    "    \"\"\"\n",
    "    print(f'---Inspecting queue with {len(queue)} queue records---\\n')\n",
    "\n",
    "    cnt = print_first_n\n",
    "\n",
    "    pop_ix = -1 if reversed_order else 0\n",
    "\n",
    "    queue = copy(queue)\n",
    "    data = []\n",
    "    while cnt > 0 and (len(data) > 0 or len(queue) > 0):\n",
    "        # ensures we load the next non-empty json in the queue when we run out of records\n",
    "        while len(data) == 0 and len(queue) > 0:\n",
    "            queue_record = queue.pop(pop_ix)\n",
    "            data = json.loads(queue_record['path'].read_text())\n",
    "\n",
    "        # in the absolute worst case scenario when we run out of records altogether we exit the function\n",
    "        if len(queue) == 0 and len(data) == 0:\n",
    "            print(f'Couldn\\'t find sufficient records, ending inspect.')\n",
    "            return\n",
    "\n",
    "        record = data.pop(pop_ix)\n",
    "\n",
    "        print(f'Record from {queue_record[\"path\"]}:\\n')\n",
    "        print(json.dumps(record, indent=4))\n",
    "        print(f'\\n\\n')\n",
    "\n",
    "        cnt -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5df36e6-bad7-47c2-9850-ce48df12283d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# inspect_queue(queue, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adcd9627-8882-4f07-9401-9c0f912fc5b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "def display_schema(df, hide_options=True):\n",
    "    schema_info = [Row(column_name=field.name, data_type=str(field.dataType), nullable=field.nullable, metadata=field.metadata) for field in df.schema.fields]\n",
    "\n",
    "    if hide_options:\n",
    "        for row in schema_info:\n",
    "            if 'options' in row['metadata']:\n",
    "                del row['metadata']['options']\n",
    "            # print(type(row['metadata']), row['metadata'])\n",
    "\n",
    "    schema_df = spark.createDataFrame(schema_info)\n",
    "    display(schema_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c7649f0-687d-404e-ac67-a7d718c6677e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display_schema(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "467cf023-9a6e-4dd2-a36a-00648570a73d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def metadata(df, col):\n",
    "#     print(f'metadata for df[\"{col}\"]:')\n",
    "#     print(json.dumps(df.schema[col].metadata, indent=4))\n",
    "\n",
    "# metadata(df, 'name')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "core_objects_standardize",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
